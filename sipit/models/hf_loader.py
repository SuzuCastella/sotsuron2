# sipit/models/hf_loader.py
import torch
from typing import Optional, Literal, Tuple
from transformers import AutoModelForCausalLM, AutoTokenizer

QuantizationMode = Literal["none", "int8", "int4", "gptq", "awq"]

def _to_torch_dtype(dtype_str: str) -> torch.dtype:
    table = {
        "float32": torch.float32,
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
    }
    if dtype_str not in table:
        raise ValueError(f"Unsupported dtype: {dtype_str}")
    return table[dtype_str]


def load_model_and_tokenizer(
    model_name: str,
    device: str = "cuda",
    dtype: str = "float32",
    quantization: QuantizationMode = "none",
    bnb_compute_dtype: str = "float16",   # 4bit/8bitã®è¨ˆç®—dtype
    bnb_quant_type: str = "nf4",          # 4bité‡å­åŒ–å­ï¼ˆnf4/ fp4ï¼‰
    trust_remote_code: bool = True,
) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
    """
    é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«å¿œã˜ã¦ CausalLM ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    quantization:
      - "none" : é€šå¸¸ãƒ­ãƒ¼ãƒ‰ï¼ˆdtypeæŒ‡å®šæœ‰åŠ¹ï¼‰
      - "int8" : bitsandbytes 8bit
      - "int4" : bitsandbytes 4bitï¼ˆbnb_quant_type, bnb_compute_dtype æŒ‡å®šå¯ï¼‰
      - "gptq" : GPTQ äº‹å‰é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«å´ãŒå¯¾å¿œã—ã¦ã„ã‚‹å‰æï¼‰
      - "awq"  : AWQ äº‹å‰é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«å´ãŒå¯¾å¿œã—ã¦ã„ã‚‹å‰æï¼‰
    """
    print(f"ğŸ”¹ Loading model: {model_name} on {device} (dtype={dtype}, quantization={quantization})")

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)
    # GPT-2 ç³»ãªã©ã¯ pad_token ãŒç„¡ã„ã“ã¨ãŒå¤šã„ã®ã§å®‰å…¨ã«è£œå®Œ
    if tokenizer.pad_token is None and tokenizer.eos_token is not None:
        tokenizer.pad_token = tokenizer.eos_token

    # device_map ã¯ GPU ã®ã¨ã "auto" ã«ã—ã¦ãŠãã¨åˆ†æ•£/ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ãŒåŠ¹ã
    device_map = "auto" if device.startswith("cuda") else None

    if quantization == "none":
        torch_dtype = _to_torch_dtype(dtype)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            dtype=torch_dtype,              # â† ã“ã¡ã‚‰ã‚’ä½¿ã†ã¨æ–°ã—ã„transformersã§è­¦å‘ŠãŒå‡ºãªã„
            device_map=device_map,
            trust_remote_code=trust_remote_code,
        )
        qdesc = f"none (dtype={torch_dtype})"

    elif quantization == "int8":
        # bitsandbytes 8bit
        # dtypeã¯ã“ã“ã§ã¯ç›´æ¥æŒ‡å®šã—ãªã„ï¼ˆbnbã®è¨ˆç®—dtypeã«å§”è­²ï¼‰
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            load_in_8bit=True,
            device_map=device_map,
            trust_remote_code=trust_remote_code,
        )
        qdesc = "bitsandbytes int8"

    elif quantization == "int4":
        # bitsandbytes 4bit
        # è¿½åŠ ã®å¼•æ•°ã¯ bnb ç³»ã®åå‰ã«åˆã‚ã›ã‚‹
        _compute_dtype = _to_torch_dtype(bnb_compute_dtype)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type=bnb_quant_type,   # "nf4" or "fp4"
            bnb_4bit_compute_dtype=_compute_dtype,
            device_map=device_map,
            trust_remote_code=trust_remote_code,
        )
        qdesc = f"bitsandbytes int4 (quant={bnb_quant_type}, compute={_compute_dtype})"

    elif quantization in ("gptq", "awq"):
        # äº‹å‰é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å‰æã¨ã—ã¦é€šå¸¸ãƒ­ãƒ¼ãƒ‰
        # å¤šãã®ãƒªãƒã§ dtype æŒ‡å®šã¯ä¸è¦/ç„¡è¦–ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ä»˜ã‘ãªã„
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map=device_map,
            trust_remote_code=trust_remote_code,
        )
        qdesc = quantization

    else:
        raise ValueError(f"Unsupported quantization mode: {quantization}")

    # éš ã‚ŒçŠ¶æ…‹ã‚’ forward æ™‚ã«è¿”ã™
    if hasattr(model, "config"):
        model.config.output_hidden_states = True
    model.eval()

    print(f"âœ… Model loaded with quantization: {qdesc}")
    return model, tokenizer
